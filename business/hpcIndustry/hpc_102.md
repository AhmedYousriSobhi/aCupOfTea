# 102 - HPC - A More Gentle Introduction
The contents in this document are from Coursera Course [**Introduction High Perfromance Computing**](https://www.coursera.org/learn/introduction-high-performance-computing), and any additional contents are from web browsering over the releam of internet.

# Table of Content
- [102 - HPC - A More Gentle Introduction](#102---hpc---a-more-gentle-introduction)
- [Table of Content](#table-of-content)
- [HPC FileSystems](#hpc-filesystems)
  - [FileSystem](#filesystem)
  - [What Belongs Where?](#what-belongs-where)
- [HPC Concepts](#hpc-concepts)
  - [HPC Architecture](#hpc-architecture)
    - [Architectue of a Supercomputer](#architectue-of-a-supercomputer)
    - [Racks](#racks)
      - [Compute Node](#compute-node)
    - [Interconnect](#interconnect)
  - [Software Installation](#software-installation)
    - [Modules](#modules)
      - [Working with Modules](#working-with-modules)
    - [Installing Your Own Software](#installing-your-own-software)
  - [Allocation](#allocation)
  - [Node Types](#node-types)
  - [Job Submissions with Slurm](#job-submissions-with-slurm)
    - [Job Scheduling](#job-scheduling)
  - [Application Profiling](#application-profiling)
  - [Parallel Memory Models](#parallel-memory-models)
    - [Visulizating Memory Models](#visulizating-memory-models)
  - [Parallelism](#parallelism)
    - [Data Parallelism](#data-parallelism)
      - [SIMD vs SPMD](#simd-vs-spmd)
      - [SIMD](#simd)
        - [How to Vectorize your Data?](#how-to-vectorize-your-data)
  - [High Throughput Computing (HTC)](#high-throughput-computing-htc)
    - [How to HTC?](#how-to-htc)
    - [Questions to highlight!](#questions-to-highlight)
- [How to Parallelize Code?](#how-to-parallelize-code)
  - [General Parallelism](#general-parallelism)
  - [What can be Parallelized?](#what-can-be-parallelized)
  - [How to Parallelized using Languages?](#how-to-parallelized-using-languages)
- [References](#references)

# HPC FileSystems
There are three main typically allocated storage space for users on HPC infrastructure, where each has an important different reasons. The three are:
- Home
- Project or Work
- Scratch

Note in case there are only two spaces, it would be Home & Scratch.
## FileSystem
|Space|Details|
|-|-|
/home|- Main purpose is for the user only, where the user is the onwer of this space, and can install/put his personal files.</br>- Usually this space is backed up.</br>Usually allocated small space like 5GB.</br>When a user login, he typically login to this space (his /home directory).</br>- To access your home directory: [~ , /home/$USER]
Project or Work|- This space is used for mid-level size data.</br>- Might have approximately 250-500 GB of space available.</br>- Typically: /projects/$USER
Scratch|- It is provided on most HPC systems.</br>- Usually a much large quota available </br>- Temporary space.</br>- Usually not backed up.</br>- cd /scratch/$USER

## What Belongs Where?
|Space|Belongs|
|-|-|
|/home|- Scripts</br>- code</br>- Very small files</br>- Inappropriate for sharing files with others</br>- Inappropriate for job output
|/projects|- Code/files/libararies relevant for any software you are installing (if you want to share files with others)</br>- Mid-level size input files</br>- Appropriate for sharing files with others</br>- Inappropriate for job output
|/scratch|- Output from running jobs</br>- Large files</br>- Appropriate for sharing files with others</br>- THIS IS NOT APPROPRIATE FOR LONG TERM STORAGE; as these data will purge after set of time, and will be lost.

# HPC Concepts
## HPC Architecture
This section aims to get an understanding of computer hardware and infrastructure. The basic components are [Processors, Memory, Network, Storage].

### Architectue of a Supercomputer
The architecture (from highest level to lowest level) is:
- System.
- Nodes.
- Cores/Memory/Storage.
Then there should be the **Network Interconnect** to interconnect the components of teh system to work as a whole one.
```
      ----------------
      | Compute Node |
      |    Storage   |
      |    Network   |
Racks ----------------
```

### Racks
The components of teh system are mounted in **racks**, which are nice cabinents with rails, can be purchased stardards or customized, and can encompass entires rooms.

The part behind racks are cooling units, to keeps optimal temparature.

The infrastructure of racks are:
- Storage - disks
- Network - high speed switches and cables
- Compute - for example: 8 chassis of 4 nodes each (32 total)

#### Compute Node 
|Component|Details|
|-|-|
CPU Sockets|- The compute nodes sets within sockets on node, under large silver heat sinks, for example (2 sockets, 2 CPUs) is a compute node.</br>- Within CPUs are cores, main processing power and memory (cache).</br>- For example, a node has 12 cores per socketed CPU, this means 24 total. </br>- The power comes from the amount of cores a compute node has.
Memory Cards|- Memory cards are eight green, thin cards (RAM).</br>- Shared Memory on node, e.g., eight 16-GB memory cards per node.</br>Also there are memory in socketed CPUS, which are cache, and they are shared between sockets.
### Interconnect
The supercomputers should work together as a one to solve a huge problem, Here comes the reason for an interconnect to make everything work together.

The reasons for interconnect:
- Access to memory and computing power.
- Nodes talk to each other.

The type of interconnect that commonly used in HPC are:
- OmniPath.
- InfiniBand.

## Software Installation
Yes, we can't do anything without application to execute the job we need, so we have to install all the application requried in our system. But managing loaded software can be a headache for that several reasons like:
- Make sure that correct versions are available.
- Make sure that software dependencies for package **A** don't interfere with package **B**.

What about a method to simply load software into a directory and start using it on our shared system?. Here comes the **Package Manager**. But this does not mean we will not use the normal software installation method, yes we will need it in some cases, but the installation should be from the source, for more details, I will explain it below.

### Modules
Modules are pieces of code that can be dynamically loaded and unloaded into the Linux kernel to extend its functionality. So environment modules allows centers to provide multiple versions of software and load dependencies seamessly.

A module is a package that contains al the files required to run the software, including libraries, and will also load the required dependencies.

The accessing of linux modules are very simple using a few commands.

#### Working with Modules
|Module Action|Details|
|-|-|
See a list of available modules| **module avail**
Load a module|- Adds software to your $PATH.</br>- May also load dependencies.</br>- May also unload other versions or dependencies that would conflict. </br>- **module load <name_of_module>/<version>**
See a list of loaded modules| **module list**
Unload a module| **module unload**
Unload all modules| **module purge**
Discover information about module|- **module spider <name_of_module>/<version>**</br>- Tells about dependencies, the package, etc.

### Installing Your Own Software
In such cases, where the software user requried is not installed, or required an older version to install that does not have a module file, or the HPC does not provide a module manager.

There are general process to follow:
- Download Software:This might have several looks, like;
  - Clone repo from Git.
  - Download a docker or singularity image.
  - Install from a file.
  - Olden days - Install from a disk.
  - Any other appropriate methods :)
- Install Software [Read Instructions, Install dependencies].
- Compile.
- Use.

## Allocation
According to [nrel documents](https://www.nrel.gov/hpc/resource-allocations.html) about allocation in HPC: 
```
An allocation is a portion of the HPC data center resources planned for a particular research project's use.
```
HPC centers provides infrastructures for enormous number of users, where each user needs to share a portion of the system. So users must justify how much of the system they need to use.

HPCs centers manage resources and keep track of current or upcoming usage by:
- Determining Priority: as certain projects may have higher priority than others.
- Reporting: like paper citation for grants 

Each HPC centers have different policies for managing allocations, so user must make sure to understand them.

## Node Types
There are three general types of nodes in each HPC system;
|Node Type|Details|
|-|-|
Login nodes|- Where you typically land when logging into the system.</br>- Not a place for heavy computation.</br>- Not a place for running memory intensive applications.</br>- There are great for: Script editting, and job submission.
Compile nodes|- Place to compile code.</br>- Same software stack and compilers as compute nodes</br>- When compile code on this node should run on compute node.</br>- Only certain languages required compiling like [C, C++, Fortran], and not for [Python, R, Matlab].
Compute nodes|- Where the submitted job run.</br>- Accessible indirectly through job schduler.</br>- Heavy computational load.

## Job Submissions with Slurm
A job is? according to [microsoft-understanding jobs and tasks](https://learn.microsoft.com/en-us/powershell/high-performance-computing/understanding-jobs-and-tasks?view=hpc19-ps);
```
In HPC pack, a job is a resource request that is submitted to the HPC Job Scheduler Service and that contains, or will contains, one or more tasks.
```

There are two types of jobs in HPC system to make your work done;
|Job type|Details|
|-|-|
Batch jobs|- Submit job that will be executed in the background. </br>- Create a text file containing information about the job.</br>- Submit the job file to a queue.
Interactive jobs|- Work interactively at the command line of a compute node.</br>- Login to compute node.

### Job Scheduling
As you are not the only one to use the supercomputer, so the job can't not just run instantly, and will be scheduled and putted in a queue until resources are available.

To do so, we need a software that will distrubte the jobs approprately and manage the resources. The most famouse one used is **Slurm**, which is stands for **Simple Linux Utility for Resource Management**. Slurm do the following:
- Keeps track of what nodes are busy/available, and what jobs are queued or running.
- Tells the resource manager when to run which job on the availalbe resources.

Most of the usefull Slurm commands is **sbatch**, refer to the [Slurm docs](https://slurm.schedmd.com/sbatch.html) for more details. ALso there are some Slurm environment variables that could be usefull to pass to job script like [$SLURM_NTASKS, $SLURM_MEM_PER_CPU].

## Application Profiling
To efficently be able to determine:
- if your app requires parallelism.
- where is the most resource consuming in your code.
- the required resources for your application.

Then it is best to try to split your codes into chunks and try to profile the behaviour of each segement in your code, starting with the loops.

## Parallel Memory Models
| Memory Model             | Description                         | Illustration                                |
|-|-|-|
| Distributed Memory Model | Each node has its own local memory. Communication via message passing. | Nodes are connected, each with local memory.   |      
| Shared Memory Model      | All processing elements share a global address space.     | Single shared memory space accessible by multiple processing elements (PEs). |
| Hybrid Memory Model      | Combination of distributed and shared memory models.     | Nodes have local memory, but PEs within a node share a common memory space.  |
| NUMA Model               | Non-Uniform Memory Access. Latency differs based on proximity to the memory.          | Nodes have local memory with varying access latency, connected by interconnect.  |                                             |
| Cache Coherent Model     | Maintains data consistency in shared memory systems.         | Multiple caches (L1, L2) coherent with each other and main memory. |
| Global Address Space Model    | Shared memory model with a global address space.  | Single global memory space accessible by all PEs.  |

### Visulizating Memory Models
- Distributed Memory Model (Message Passing Model):
  ```bash
  Node 1        Node 2        Node 3
  +--------+    +--------+    +--------+
  | Memory |    | Memory |    | Memory |
  +--------+    +--------+    +--------+
  |        |    |        |    |        |
  |   PE   |    |   PE   |    |   PE   |
  |        |    |        |    |        |
  +--------+    +--------+    +--------+
  ```
- Shared Memory Model:
  ```bash
  +--------+
  | Memory |
  +--------+
  |   PE   |
  +--------+
  |   PE   |
  +--------+
  |   PE   |
  +--------+
  ```
- Hybrid Memory Model:
  ```bash
  Node 1        Node 2        Node 3
  +--------+    +--------+    +--------+
  | Memory |    | Memory |    | Memory |
  +--------+    +--------+    +--------+
  |        |    |        |    |        |
  |   PE   |    |   PE   |    |   PE   |
  |        |    |        |    |        |
  +--------+    +--------+    +--------+
      |            |            |
      +------------+------------+
          Shared Memory
  ```
- Non-Uniform Memory Access (NUMA) Model:
  ```bash
  Node 1               Node 2
  +--------+           +--------+
  | Local  |           | Local  |
  | Memory |<--------->| Memory |
  +--------+           +--------+
  |        |           |        |
  |   PE   |           |   PE   |
  |        |           |        |
  +--------+           +--------+
  ```
- Cache Coherent Memory Model:
  ```bash
  +--------+    +--------+
  | Cache  |    | Cache  |
  |   L1   |    |   L1   |
  +--------+    +--------+
      |            |
  +--------+    +--------+
  | Cache  |    | Cache  |
  |   L2   |    |   L2   |
  +--------+    +--------+
      |            |
  +--------+    +--------+
  | Memory |    | Memory |
  +--------+    +--------+
  |   PE   |    |   PE   |
  +--------+    +--------+
  ```
- Global Address Space Model:
  ```bash
  +--------+    +--------+    +--------+
  | Memory |    | Memory |    | Memory |
  +--------+    +--------+    +--------+
  |   PE   |    |   PE   |    |   PE   |
  +--------+    +--------+    +--------+
  ```
  
## Parallelism
There are two types of parallelism;

|      Parallelism    |              Data Parallelism         |            Task Parallelism          |
|-|-|-|
| Definition          | Involves dividing a task based on the data, performing the same operation on multiple data elements concurrently | Involves dividing a program into independent tasks, each executed  concurrently |
| Work Division       | Division based on data elements. Each processing unit works on different subset of data. | Division based on independent tasks. Different processing units handle different tasks.  |
| Example             | Parallel processing of elements in a dataset. Vectorized computations.  | Concurrent execution of different functions/modules in a program.  |
| Independence        | Assumes independence of data elements (operations on one element do not affect others).  | Assumes independence of tasks. Tasks can be executed independently without dependencies.      |
| Coordination        | Synchronization may be required if data elements interact or share state.    | Less need for synchronization as tasks are often independent.  |
| Utilization         | Efficient for operations on large datasets or arrays.    | Useful when different parts of a program can run concurrently.     |
| Real-world Usage    | Image processing, numerical simulations with large datasets. | Parallel execution of different modules in a software application.     |

There are additional types of parallelism, you can take a look at, they are: [*Pipeline Parallelism*, *Bit-level Parallelism*]

### Data Parallelism 
In data parallelism, they are divided into two types;
- SIMD: Single Instruction, Multiple Data.
- SPMD: Single Program, Multiple Data.
#### SIMD vs SPMD
|Data Parallelism Type|SIMD|SPMD|
|-|-|-|
Definition|SIMD is a type of data parallelism where a single instruction is applied to multiple data elements simultaneously| SPDM is a type of data parallelism where multiple instances of the same program are executed concurrently, but each instance may operate on different data.
How it Works|A single instruction is broadcast to all processing units, and each unit operates on its local set of data. All processing units execute the same instruction in parallel on their respective data elements.|Each processing unit executes the same program, but they may process different data subsets. Unlike SIMD, where all units execute the exact same instruction simultaneously, in SPMD, there can be conditional statements allowing different processing units to follow different paths in the program.
Examples|Modern processors often have SIMD instructions that enable parallel processing of multiple data elements in a single instruction. For instance, Intel's SSE (Streaming SIMD Extensions) and AVX (Advanced Vector Extensions) are SIMD instruction sets.|In parallel programming models like CUDA or OpenCL, SPMD is commonly used. Each parallel thread executes the same kernel (program) but may operate on different portions of the data.
Benefits|SIMD is particularly efficient for operations where the same operation needs to be applied to a large set of independent data elements.| SPMD is flexible and allows for more varied processing on different data elements. It is suitable for applications where different processing units may follow different execution paths based on data-dependent conditions.

#### SIMD
The SIMD is mainly a vector operation like [addition, subtraction, multiplication, division], so it is best to have our data vectorized!.

##### How to Vectorize your Data?
Vectorization aims to involves converting scalar code, which operates on signle data elements, into vectorized code for efficient parallel processing.

## High Throughput Computing (HTC)
Let's image the following scenario, where we have a parallel job, which will be executed into two nodes, now each of these parallel jobs consume only a small part of the node, and the rest are idle.
```diagram
        Parallel Job 
    ---------    ---------
    | Job-A |    | Job-B |
    ---------    ---------

        Compute Nodes 
    ---------    ---------
    | Node-A |    | Node-B |
    |        |    |        |
    |        |    |        |
    ----------    ----------

```

Here comes **HTC** to the rescue!!
- HTC useful when have many small jobs that require little computational power or memory; "Multiple small jobs spread across many processors".
- Jobs are typically serial, and not parallel.
- Small serial jobs can fill in the 'gaps' left by large parallel jobs.
- Which will leads to *effictively parallel* where batch of jobs completes faster when spread across multiple cores.

### How to HTC?
- [Task Parallelsim] Break down your problem and then submit a lot of smaller jobs.
- [Job Scheduling] The resource managere (i.e., Slurm) should take care of the rest.

### Questions to highlight!
1. Is HTC appropriate? Actually it is a problem dependent!
2. Is the serial execution time reasonanble?
3. Can the problem fit into one core's worth of memory?

# How to Parallelize Code?
## General Parallelism
- You can parallelize code across cores on one node using shared memory or across nodes using distrubuted memory.
- Generally it's easier to parallelize using shared memory than distributed memory; means try first to know how to parallelism across cores rather than across nodes.
- Parallel prgoramming is either supported directly by the compiler or by libraries that will help you parallelize your code.
## What can be Parallelized?
Code that can be executed independently of other instructions.
  - In a loop, make sure iteration do not depend on other iteration.
    ```python
    # Dependancy - not parallel:
    for i=1:10
      a[i]= b[i] + a[i-1]
    end

    # or 
    # Also dependancy - not parallel:
    for i=1:10
      a[i]= b[i] + c[i]
      d[i]= e[i] * a[i]
    end

    ```
  - Search for different algorithms or functions instead that are intended or suitable for parallelization.
## How to Parallelized using Languages?
There are two types of languages that could support parallelism;
1. Compiled Langauages [C++, Fortran, etc] use:
   1. OpenMP "Open Multi-Processing": Shared memory parallelization (within a node).
   2. MPI "Message Passing Interface": Parallelize across nodes.
2. Scripted Language (Python, R):
   1. Packages that you can use that have functions that will allow you to parallelize based on certain bits of code.

# References
- [Parallel Programming Fundamental](https://curc.readthedocs.io/en/latest/programming/parallel-programming-fundamentals.html#mpi) - by Research Computing University of Colorado Boulder.
- [MPI tutorial](https://mpitutorial.com/tutorials/mpi-introduction/) - by MPItutorial Wes Kendall.
- [Parallel Programming Primer](https://researchcomputing.princeton.edu/support/knowledge-base/parallel-code) - by Princeton Research Computing.
